{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. Merging Datasets\n",
        "\n",
        "Merging is your new favourite word. Merging is one of the most powerful skills in data analysis, and it is what separates the little girlsÂ â€“Â with their Excel tablesÂ â€“Â from big powerful boss women like you.\n",
        "\n",
        "It's hard to convey this in words, so let us get right to examples. \n",
        "\n",
        "Let's load the IDS table and the clean stroke count table that Marina so kindly cleaned for us in the last notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "IDS Table shape: (161368, 3)\n",
            "\n",
            "Stroke Count Table shape: (89586, 3)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the IDS table\n",
        "ids_df = pd.read_csv('../daniel_tables/ids_df.csv',\n",
        "    index_col=None,\n",
        "    encoding='utf-8')\n",
        "\n",
        "# cleaning\n",
        "ids_df = ids_df.drop_duplicates()\n",
        "ids_df = ids_df.dropna(subset=['character', 'components'], how='any')\n",
        "ids_df = ids_df[~ids_df['components'].str.contains('â†')]\n",
        "ids_df = ids_df[~ids_df['components'].str.contains('â†’')]\n",
        "ids_df = ids_df[~ids_df['components'].str.contains('CDP')].copy()\n",
        "\n",
        "# Load the clean stroke count table\n",
        "stroke_df = pd.read_csv('../marina_tables/stroke_count_clean.csv',\n",
        "    index_col=None,\n",
        "    encoding='utf-8')\n",
        "\n",
        "# Print the two tables' shapes\n",
        "print(f\"\\nIDS Table shape: {ids_df.shape}\")\n",
        "print(f\"\\nStroke Count Table shape: {stroke_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have two tables. The IDS table has character decomposition information, and the stroke count table has stroke counts. Wouldn't it be nice to have all of that on one table?\n",
        "\n",
        "That's what merging is for! But here's the thing: not every character in the IDS table has a stroke count, and not every character in the stroke count table has IDS decomposition data. So we need to decide: what do we want to keep?\n",
        "\n",
        "Pandas gives us several options, and they're called \"joins\". Let's see what happens with each one:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inner Join: Only Matching Characters\n",
        "\n",
        "An inner join keeps ONLY the rows where the character exists in BOTH tables. This is the most restrictive option - if a character doesn't have a stroke count, it gets dropped. If a character doesn't have IDS data, it gets dropped.\n",
        "\n",
        "Use this when you only want complete data - characters that have information in both tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original IDS table: 161368 rows\n",
            "Original stroke count table: 89586 rows\n",
            "After inner join: 151660 rows\n",
            "\n",
            "Lost 9708 rows from IDS table\n",
            "\n",
            "First few rows:\n",
            "  code_point_x character components code_point_y stroke_count\n",
            "0      U+04E00         ä¸€          ä¸€       U+4E00            1\n",
            "1      U+0620C         æˆŒ        â¿µæˆŠä¸€       U+620C            6\n",
            "2      U+06B72         æ­²        â¿»æ­¥æˆŒ       U+6B72           13\n",
            "3      U+0528C         åŠŒ        â¿°æ­²åˆ‚       U+528C           15\n",
            "4      U+05666         å™¦        â¿°å£æ­²       U+5666           16\n"
          ]
        }
      ],
      "source": [
        "# Inner join: only keep characters that exist in BOTH tables\n",
        "df_inner = pd.merge(ids_df, stroke_df,\n",
        "                    on='character',\n",
        "                    how='inner')\n",
        "\n",
        "print(f\"Original IDS table: {len(ids_df)} rows\")\n",
        "print(f\"Original stroke count table: {len(stroke_df)} rows\")\n",
        "print(f\"After inner join: {len(df_inner)} rows\")\n",
        "print(f\"\\nLost {len(ids_df) - len(df_inner)} rows from IDS table\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_inner.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Left Join: Keep All IDS Characters\n",
        "\n",
        "A left join keeps ALL rows from the LEFT table (ids_df) and adds matching data from the RIGHT table (stroke_df) where it exists. If there's no match, the stroke count columns will be empty (NaN).\n",
        "\n",
        "Use this when you want to keep all your IDS data, even if some characters don't have stroke counts yet.\n",
        "\n",
        "**Note:** When each table has an identically-named column, if you don't merge on that column, you get _x and _y added to the end. In this case, we can't merge on the code_point column, because the one has an extra 0 in the codes. Instead, we will delete this from one of the tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original IDS table: 161368 rows\n",
            "After left join: 161368 rows\n",
            "\n",
            "Kept all 161368 rows from IDS table!\n",
            "Added stroke counts for 151660 characters\n",
            "Missing stroke counts for 9708 characters\n",
            "\n",
            "First few rows:\n",
            "  code_point character components stroke_count\n",
            "0    U+04E00         ä¸€          ä¸€            1\n",
            "1    U+0620C         æˆŒ        â¿µæˆŠä¸€            6\n",
            "2    U+06B72         æ­²        â¿»æ­¥æˆŒ           13\n",
            "3    U+0528C         åŠŒ        â¿°æ­²åˆ‚           15\n",
            "4    U+05666         å™¦        â¿°å£æ­²           16\n",
            "\n",
            "Some rows with missing stroke counts:\n",
            "      code_point character components stroke_count\n",
            "32884     U+9FEB         é¿«     â¿¹æ°”å¥¥[G]          NaN\n",
            "32885     U+9FEC         é¿¬        â¿°çŸ³ç”°          NaN\n",
            "32886     U+9FED         é¿­        â¿°é’…å°”          NaN\n",
            "32887     U+9FEE         é¿®        â¿¸å¹¿å¸¯          NaN\n",
            "32888     U+9FEF         é¿¯      â¿±ç‰©â¿±äººç±³          NaN\n"
          ]
        }
      ],
      "source": [
        "# Delete code_point column from stroke_df\n",
        "stroke_df = stroke_df.drop(columns=['code_point'])\n",
        "\n",
        "# Left join: keep all IDS characters, add stroke counts where available\n",
        "df_left = pd.merge(ids_df, stroke_df,\n",
        "                   on='character',\n",
        "                   how='left')\n",
        "\n",
        "print(f\"Original IDS table: {len(ids_df)} rows\")\n",
        "print(f\"After left join: {len(df_left)} rows\")\n",
        "print(f\"\\nKept all {len(ids_df)} rows from IDS table!\")\n",
        "print(f\"Added stroke counts for {df_left['stroke_count'].notna().sum()} characters\")\n",
        "print(f\"Missing stroke counts for {df_left['stroke_count'].isna().sum()} characters\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_left.head())\n",
        "print(\"\\nSome rows with missing stroke counts:\")\n",
        "print(df_left[df_left['stroke_count'].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Right Join: Keep All Stroke Count Characters\n",
        "\n",
        "A right join keeps ALL rows from the RIGHT table (stroke_df) and adds matching data from the LEFT table (ids_df) where it exists. If there's no match, the IDS columns will be empty (NaN).\n",
        "\n",
        "Use this when you want to keep all your stroke count data, even if some characters don't have IDS decomposition yet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original stroke count table: 89586 rows\n",
            "After right join: 151936 rows\n",
            "\n",
            "Kept all 89586 rows from stroke count table!\n",
            "Added IDS data for 151660 characters\n",
            "Missing IDS data for 276 characters\n",
            "\n",
            "First few rows:\n",
            "  code_point character components stroke_count\n",
            "0     U+03B1         Î±          Î±            1\n",
            "1     U+2113         â„“          â„“            1\n",
            "2     U+2460         â‘           â‘             1\n",
            "3     U+2461         â‘¡          â‘¡            2\n",
            "4     U+2462         â‘¢          â‘¢            3\n",
            "\n",
            "Some rows with missing IDS data:\n",
            "       code_point   character components stroke_count\n",
            "18            NaN           â‘°        NaN           17\n",
            "19            NaN           â‘±        NaN           18\n",
            "21            NaN           â‘³        NaN           20\n",
            "151286        NaN  &CDP-854C;        NaN            8\n",
            "151288        NaN  &CDP-8551;        NaN            7\n"
          ]
        }
      ],
      "source": [
        "# Right join: keep all stroke count characters, add IDS data where available\n",
        "df_right = pd.merge(ids_df, stroke_df,\n",
        "                    on='character',\n",
        "                    how='right')\n",
        "\n",
        "print(f\"Original stroke count table: {len(stroke_df)} rows\")\n",
        "print(f\"After right join: {len(df_right)} rows\")\n",
        "print(f\"\\nKept all {len(stroke_df)} rows from stroke count table!\")\n",
        "print(f\"Added IDS data for {df_right['components'].notna().sum()} characters\")\n",
        "print(f\"Missing IDS data for {df_right['components'].isna().sum()} characters\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df_right.head())\n",
        "print(\"\\nSome rows with missing IDS data:\")\n",
        "print(df_right[df_right['components'].isna()].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing the Results\n",
        "\n",
        "OK Marina, let's see what we learned. Why are the sizes different?\n",
        "\n",
        "The key is understanding what each join type keeps:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary of merge results:\n",
            "============================================================\n",
            "Original IDS table:           161,368 rows\n",
            "Original stroke count table:   89,586 rows\n",
            "============================================================\n",
            "Inner join:  151,660 rows (only characters in BOTH tables)\n",
            "Left join:   161,368 rows (all IDS + matching stroke counts)\n",
            "Right join:  151,936 rows (all stroke counts + matching IDS)\n",
            "============================================================\n",
            "\n",
            "Why is inner join smaller?\n",
            "  â†’ Because 9,708 characters in IDS table don't have stroke counts\n",
            "  â†’ And -62,074 characters in stroke count table don't have IDS data\n",
            "\n",
            "Why are left and right joins different sizes?\n",
            "  â†’ Left join keeps all 161,368 IDS characters (some without stroke counts)\n",
            "  â†’ Right join keeps all 89,586 stroke count characters (some without IDS data)\n",
            "  â†’ They're different because the two tables have different characters!\n"
          ]
        }
      ],
      "source": [
        "# Compare the sizes\n",
        "print(\"Summary of merge results:\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Original IDS table:           {len(ids_df):,} rows\")\n",
        "print(f\"Original stroke count table:   {len(stroke_df):,} rows\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Inner join:  {len(df_inner):,} rows (only characters in BOTH tables)\")\n",
        "print(f\"Left join:   {len(df_left):,} rows (all IDS + matching stroke counts)\")\n",
        "print(f\"Right join:  {len(df_right):,} rows (all stroke counts + matching IDS)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nWhy is inner join smaller?\")\n",
        "print(f\"  â†’ Because {len(ids_df) - len(df_inner):,} characters in IDS table don't have stroke counts\")\n",
        "print(f\"  â†’ And {len(stroke_df) - len(df_inner):,} characters in stroke count table don't have IDS data\")\n",
        "print(f\"\\nWhy are left and right joins different sizes?\")\n",
        "print(f\"  â†’ Left join keeps all {len(ids_df):,} IDS characters (some without stroke counts)\")\n",
        "print(f\"  â†’ Right join keeps all {len(stroke_df):,} stroke count characters (some without IDS data)\")\n",
        "print(f\"  â†’ They're different because the two tables have different characters!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## To join me, you must join yourself\n",
        "\n",
        "Perfect! Now you understand the difference. For most of your work, you'll probably want to use **left join** to keep all your IDS data and add stroke counts where available.\n",
        "\n",
        "Of course, merging isn't just for combining different tables - you can also merge a table **onto itself**. \n",
        "\n",
        "Why the hell would you want to do that? Because it's metal. ğŸ¦„ WAAAAAAAAAAAAAAAAGH!\n",
        "\n",
        "You don't believe me? Just fucking watch. \n",
        "\n",
        "OK, so you want to build something like [jisho.org](https://jisho.org/) for the character composer, right? Here's the problem, [jisho.org](https://jisho.org/) breaks characters into the 200 something radicals, but the IDS data divides things into semantic + phonetic, and the phonetic would need to be broken down further, right?  \n",
        "\n",
        "First, let's define what we mean by \"radicals\".\n",
        "\n",
        "- Go to [jisho.org](https://jisho.org/), and copy all the radicals from their little tray\n",
        "- Paste into gedit\n",
        "- Open find and replace\n",
        "- Use regex to remove all numbers \n",
        "- Then, replace each word with that word surrounded by quotation marks and followed by a comma and a space.\n",
        "- put brakets around it, and you have yourself a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make a list of standard radicals from Jisho.org\n",
        "radical_list = [\n",
        "    'ä¸€', 'ï½œ', 'ä¸¶', 'ãƒ', 'ä¹™', 'äº…', 'äºŒ', 'äº ', 'äºº', 'âº…', 'ğ †¢', 'å„¿', 'å…¥', 'ãƒ', 'ä¸·', 'å†‚', 'å†–', 'å†«', \n",
        "    'å‡ ', 'å‡µ', 'åˆ€', 'âº‰', 'åŠ›', 'å‹¹', 'åŒ•', 'åŒš', 'å', 'åœ', 'å©', 'å‚', 'å¶', 'åˆ', 'ãƒ', 'ä¹', 'ãƒ¦', 'ä¹ƒ', \n",
        "    'ğ ‚‰', 'â»Œ', 'å£', 'å›—', 'åœŸ', 'å£«', 'å¤‚', 'å¤•', 'å¤§', 'å¥³', 'å­', 'å®€', 'å¯¸', 'å°', 'âºŒ', 'å°¢', 'å°¸', 'å±®', \n",
        "    'å±±', 'å·', 'å·›', 'å·¥', 'å·²', 'å·¾', 'å¹²', 'å¹º', 'å¹¿', 'å»´', 'å»¾', 'å¼‹', 'å¼“', 'ãƒ¨', 'å½‘', 'å½¡', 'å½³', 'âº–', \n",
        "    'âº˜', 'âº¡', 'âº¨', 'âº¾', 'â»', 'â»–', 'ä¹Ÿ', 'äº¡', 'åŠ', 'ä¹…', 'âº¹', 'å¿ƒ', 'æˆˆ', 'æˆ¸', 'æ‰‹', 'æ”¯', 'æ”µ', 'æ–‡', 'æ–—', \n",
        "    'æ–¤', 'æ–¹', 'æ— ', 'æ—¥', 'æ›°', 'æœˆ', 'æœ¨', 'æ¬ ', 'æ­¢', 'æ­¹', 'æ®³', 'æ¯”', 'æ¯›', 'æ°', 'æ°”', 'æ°´', 'ç«', 'âº£', \n",
        "    'çˆª', 'çˆ¶', 'çˆ»', 'çˆ¿', 'ç‰‡', 'ç‰›', 'çŠ¬', 'âº­', 'ç‹', 'å…ƒ', 'äº•', 'å‹¿', 'å°¤', 'äº”', 'å±¯', 'å·´', 'æ¯‹', 'ç„', \n",
        "    'ç“¦', 'ç”˜', 'ç”Ÿ', 'ç”¨', 'ç”°', 'ç–‹', 'ç–’', 'ç™¶', 'ç™½', 'çš®', 'çš¿', 'ç›®', 'çŸ›', 'çŸ¢', 'çŸ³', 'ç¤º', 'ç¦¸', 'ç¦¾', \n",
        "    'ç©´', 'ç«‹', 'â»‚', 'ä¸–', 'å·¨', 'å†Š', 'æ¯', 'âº²', 'ç‰™', 'ç“œ', 'ç«¹', 'ç±³', 'ç³¸', 'ç¼¶', 'ç¾Š', 'ç¾½', 'è€Œ', 'è€’', \n",
        "    'è€³', 'è¿', 'è‚‰', 'è‡ª', 'è‡³', 'è‡¼', 'èˆŒ', 'èˆŸ', 'è‰®', 'è‰²', 'è™', 'è™«', 'è¡€', 'è¡Œ', 'è¡£', 'è¥¿', 'è‡£', 'è¦‹', \n",
        "    'è§’', 'è¨€', 'è°·', 'è±†', 'è±•', 'è±¸', 'è²', 'èµ¤', 'èµ°', 'è¶³', 'èº«', 'è»Š', 'è¾›', 'è¾°', 'é…‰', 'é‡†', 'é‡Œ', 'èˆ›', \n",
        "    'éº¦', 'é‡‘', 'é•·', 'é–€', 'éš¶', 'éš¹', 'é›¨', 'é’', 'é', 'å¥„', 'å²¡', 'å…', 'æ–‰', 'é¢', 'é©', 'éŸ­', 'éŸ³', 'é ', \n",
        "    'é¢¨', 'é£›', 'é£Ÿ', 'é¦–', 'é¦™', 'å“', 'é¦¬', 'éª¨', 'é«˜', 'é«Ÿ', 'é¬¥', 'é¬¯', 'é¬²', 'é¬¼', 'ç«œ', 'éŸ‹', 'é­š', 'é³¥', \n",
        "    'é¹µ', 'é¹¿', 'éº»', 'äº€', 'å•‡', 'é»„', 'é»’', 'é»', 'é»¹', 'ç„¡', 'æ­¯', 'é»½', 'é¼', 'é¼“', 'é¼ ', 'é¼»', 'é½Š', 'é¾ '\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Shit, after testing, Daniel found that a good half of radicals went missing, so we need to add some things. Here is what Daniel came up with quickly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's add missing stuff\n",
        "radical_list += [\n",
        "    'é˜', 'é‡’', 'äº»', 'è‰¹', 'å¿„', 'æ‰Œ', 'æ°µ', 'çŠ­', 'ç¤»', 'ç½’', 'è¾¶', 'ç‰œ', 'å…«', 'ä¸¨', 'ç½‘', 'ä¸¿', 'åˆ‚', 'ğ ‚†',\n",
        "    'é»‘', 'ğ§¾·', 'é¾œ', 'é¾', 'æ”´'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Level 1: More cleaning\n",
        "\n",
        "First, we need to clean up the `components` column, distinguish between things that are and are not 'radicals', and offload contents to new columns. There are clever ways to do this, but we're in a hurry, so we'll do stuff the stupid way using the tools and knowledge that you've already acquired.\n",
        "\n",
        "Two extra things:\n",
        "\n",
        "- A DataFrame is basically a list of dictionaries. It can be turned into just that, and it can be remade from just that.\n",
        "- We can iterrate through DataFrame rows with `.iterrows()` and convert rows to dictionaries with `.to_dict()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "First few rows with extracted radicals:\n",
            "  character components radical_string all_components    a\n",
            "0         ä¸€          ä¸€              ä¸€              ä¸€  NaN\n",
            "1         æˆŒ        â¿µæˆŠä¸€              ä¸€             æˆŠä¸€    æˆŠ\n",
            "2         æ­²        â¿»æ­¥æˆŒ                            æ­¥æˆŒ    æ­¥\n",
            "3         åŠŒ        â¿°æ­²åˆ‚              åˆ‚             æ­²åˆ‚    æ­²\n",
            "4         å™¦        â¿°å£æ­²              å£             å£æ­²    æ­²\n",
            "5         å¥¯        â¿±å¤§æ­²              å¤§             å¤§æ­²    æ­²\n",
            "6         æ¿Š        â¿°æ°µæ­²              æ°µ             æ°µæ­²    æ­²\n",
            "7         ç©¢        â¿°ç¦¾æ­²              ç¦¾             ç¦¾æ­²    æ­²\n",
            "8         ç¿½        â¿°æ­²ç¾½              ç¾½             æ­²ç¾½    æ­²\n",
            "9         è–‰        â¿±è‰¹æ­²              è‰¹             è‰¹æ­²    æ­²\n",
            "12395/99016 rows are completely broken down into radicals\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# TODO There are duplicates in the character column, which should be manually cleaned\n",
        "# For the moment, we'll just drop them\n",
        "ids_df = ids_df.drop_duplicates(subset=['character'])\n",
        "\n",
        "# Empty list to fill with dictionaries\n",
        "ls = []\n",
        "\n",
        "# Letters to use as column names\n",
        "letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for idx, row in ids_df.iterrows():\n",
        "    # Set letter index counter to 0\n",
        "    letter_idx = 0\n",
        "    # Convert the row to a dictionary\n",
        "    d = row.to_dict()\n",
        "    # Extract characters from the components column\n",
        "    components = row['components'] \n",
        "    # Remove structure characters\n",
        "    structure_chars = 'â¿°â¿±â¿²â¿³â¿´â¿µâ¿¶â¿·â¿¸â¿¹â¿ºâ¿»'\n",
        "    components = re.sub(f'[{structure_chars}]', '', components)\n",
        "    # Add a radical string to the row dictionary\n",
        "    d['radical_string'] = ''\n",
        "    # Iterate through the characters\n",
        "    for char in components:\n",
        "        # If the character is in the list,\n",
        "        if char in radical_list:\n",
        "            # Then add it to the radical string\n",
        "            d['radical_string'] += char\n",
        "        # If the character ISN'T in the list,\n",
        "        else:\n",
        "            # Find the current letter from the letter index using slices\n",
        "            letter = letters[letter_idx]\n",
        "            # Add the character as the value of that key\n",
        "            d[letter] = char\n",
        "            # Increment the letter index counter\n",
        "            letter_idx += 1\n",
        "    # Append the dictionary to the list\n",
        "    ls.append(d)\n",
        "\n",
        "# Convert the list of dictionaries back to a DataFrame\n",
        "ids_df = pd.DataFrame(ls)\n",
        "\n",
        "# Fill NaN with empty string\n",
        "ids_df['radical_string'] = ids_df['radical_string'].fillna('')\n",
        "\n",
        "# Create all_components column from the original components column\n",
        "# This removes structure characters but keeps all component characters (radicals + non-radicals)\n",
        "def extract_all_components(components_str):\n",
        "    \"\"\"Extract all characters from components, removing only structure characters\"\"\"\n",
        "    if pd.isna(components_str):\n",
        "        return ''\n",
        "    structure_chars = 'â¿°â¿±â¿²â¿³â¿´â¿µâ¿¶â¿·â¿¸â¿¹â¿ºâ¿»'\n",
        "    # Remove structure characters and arrows, keep everything else\n",
        "    cleaned = re.sub(f'[{structure_chars}â†’â†]', '', str(components_str))\n",
        "    return cleaned\n",
        "\n",
        "ids_df['all_components'] = ids_df['components'].apply(extract_all_components)\n",
        "\n",
        "print(\"\\nFirst few rows with extracted radicals:\")\n",
        "print(ids_df[['character', 'components', 'radical_string', 'all_components', 'a']].head(10))\n",
        "temp = ids_df[ids_df['a'].isna()]\n",
        "print(f\"{len(temp)}/{len(ids_df)} rows are completely broken down into radicals\")\n",
        "\n",
        "# Back up\n",
        "bu = ids_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fuck! That took Daniel's machine 7 seconds to calculate. That is an *eternity*. We'll be smarter next time so as not to waste all those precious seconds we could have used to think about important things, like email.\n",
        "\n",
        "Anyways, now we have Level 1 radicals - the radicals that appear directly in each character's components. But what about characters that are made up of other characters, which are in turn made up of radicals? We need to go deeper!\n",
        "\n",
        "### Level 2: Components of Components\n",
        "\n",
        "Now we're going to merge the table onto itself. For each character in the components list that isn't a radical, we'll look up its components and extract radicals from those.\n",
        "\n",
        "This is where self-joining gets interesting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows:\n",
            "      character components radical_string\n",
            "6584          ğ¨œ·        â¿°æ—é˜              é˜\n",
            "25270         ã£¸        â¿°å½³ã½”              å½³\n",
            "94264         ğ±Œ±        â¿°é½¿æ›³               \n",
            "38971         ğ¢™»  â¿±ğ¦˜’â¿°å¿ƒåŠ›[UT]             å¿ƒåŠ›\n",
            "76710         ğ¬›§        â¿°ğ«‡†å­              å­\n",
            "81507         ğ­©½        â¿°æœ¨ç±³            æœ¨ç±³é­š\n",
            "91131         ğ°›®        â¿°æ°µå¤              æ°µ\n",
            "70858         ğª½º        â¿¸ç—±æ°—               \n",
            "50632         ğ¥¢­        â¿°ç¦¾æ•              ç¦¾\n",
            "87964         ğ¡­      â¿±â‘§â¿°èˆŒå¯¸             èˆŒå¯¸\n",
            "81563/99016 rows are completely broken down into radicals\n"
          ]
        }
      ],
      "source": [
        "# Load backup\n",
        "ids_df = bu.copy()\n",
        "\n",
        "# We're going to make this a function, because we will reuse it later\n",
        "\n",
        "def go_deeper(df, col):\n",
        "    \"\"\"\n",
        "    Get radicals from next level (components of components)\n",
        "    param df: DataFrame to work with\n",
        "    param col: column to work with\n",
        "    returns df: DataFrame to work with\n",
        "    \"\"\"\n",
        "    # Reduce DataFrame to columns we need and make copy\n",
        "    cc = df[[col, 'radical_string']].copy()\n",
        "    # Fill empty radical_string with the character\n",
        "    cc['radical_string'] = cc['radical_string'].fillna('')\n",
        "    # Drop empty rows\n",
        "    cc = cc[cc['radical_string'] != ''].copy()\n",
        "    cc = cc.dropna(subset=col)\n",
        "    cc = cc[cc[col] != ''].copy()\n",
        "    # Rename columns\n",
        "    cc = cc.rename(columns={col: 'character', 'radical_string': 'new_rads'})\n",
        "    # Deduplicate again TODO this might be fucked\n",
        "    cc = cc.drop_duplicates(subset=['character'])\n",
        "    # Merge with original DataFrame\n",
        "    df = pd.merge(df, cc, on='character', how='left')\n",
        "    # Fill empty radical_string with the character\n",
        "    df['new_rads'] = df['new_rads'].fillna(value='')\n",
        "    # Split into those with and without new_rads\n",
        "    df_with = df[df['new_rads'] != ''].copy()\n",
        "    df_without = df[df['new_rads'] == ''].copy()\n",
        "    # Add new_rads to radical_string\n",
        "    df_with['radical_string'] = df_with['radical_string'] + df_with['new_rads']\n",
        "    # For df_without, the component character doesn't have radicals in our lookup\n",
        "    # We should NOT add the component character itself to radical_string (it's not a radical!)\n",
        "    # So we keep radical_string unchanged for df_without\n",
        "    # Concatenate the two DataFrames\n",
        "    df = pd.concat([df_with, df_without])\n",
        "    # Drop new_rads column\n",
        "    df = df.drop(columns=['new_rads', col])\n",
        "    # Drop duplicates\n",
        "    df = df.drop_duplicates()\n",
        "    # Fill empty radical_string with the character\n",
        "    df['radical_string'] = df['radical_string'].fillna('')\n",
        "    # Return the DataFrame\n",
        "    return df\n",
        "\n",
        "# Apply the function to column a\n",
        "ids_df = go_deeper(ids_df, 'a')\n",
        "\n",
        "print(\"First few rows:\")\n",
        "print(ids_df[['character', 'components', 'radical_string']].sample(10))\n",
        "temp = ids_df[ids_df['b'].isna()]\n",
        "print(f\"{len(temp)}/{len(ids_df)} rows are completely broken down into radicals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Level 3: Components of components of components\n",
        "\n",
        "Excellent! Let's keep going then:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows:\n",
            "      character components radical_string\n",
            "61195         ğ¨„        â¿±å‹™è¶³              è¶³\n",
            "89978         ğ°‰        â¿´å›—çˆ¿             å›—çˆ¿\n",
            "12183         æ¾†        â¿°æ°µå ¯              æ°µ\n",
            "34671         ğ ¢¬        â¿±ç„¡åŠ›             ç„¡åŠ›\n",
            "71204         ğª³‡        â¿°æœ¨è¿¦              æœ¨\n",
            "80636         ğ­²        â¿°å¤ç•ª               \n",
            "80275         ğ­Š±        â¿°å£è·›              å£\n",
            "44587         ğ£™»        â¿°æœ¨è†              æœ¨\n",
            "95081         ğ±™²    â¿°â¿±éŸ­äº•â¿±ä¸®å¥³            éŸ­äº•å¥³\n",
            "66987         ğ©©º        â¿°éª¨æ³‰              éª¨\n",
            "96380/99016 rows are completely broken down into radicals\n"
          ]
        }
      ],
      "source": [
        "# Apply the function to column b\n",
        "ids_df = go_deeper(ids_df, 'b')\n",
        "\n",
        "print(\"First few rows:\")\n",
        "print(ids_df[['character', 'components', 'radical_string']].sample(10))\n",
        "temp = ids_df[ids_df['c'].isna()]\n",
        "print(f\"{len(temp)}/{len(ids_df)} rows are completely broken down into radicals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One more time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows:\n",
            "      character components radical_string\n",
            "47716         ğ¤’‚        â¿°ğ¤‰­éš¹              éš¹\n",
            "53935         ğ¦€¨        â¿°ç³¹é‚£               \n",
            "6890          ã«§        â¿±æ—¥ç±³            æ—¥ç±³æœ¨\n",
            "16252         ç¹¡        â¿°ç³¸è‚…              ç³¸\n",
            "4909          å…Š        â¿±å…¬å„¿             å„¿äº \n",
            "21607         ç†©        â¿°ç«æ‰ˆ              ç«\n",
            "37819         ğ¡¯        â¿°å¥³éš¶             å¥³éš¶\n",
            "85594         ğ®¤¹        â¿°é˜ä¹‡              é˜\n",
            "77447         ğ¬š¦        â¿°è€³è¯              è€³\n",
            "98835         ğ®¶¨      â¿±è‰¹â¿µé—¨ç¦¾             è‰¹ç¦¾\n",
            "97289/99016 rows are completely broken down into radicals\n"
          ]
        }
      ],
      "source": [
        "# Apply the function to column c\n",
        "ids_df = go_deeper(ids_df, 'c')\n",
        "\n",
        "print(\"First few rows:\")\n",
        "print(ids_df[['character', 'components', 'radical_string']].sample(10))\n",
        "temp = ids_df[ids_df['d'].isna()]\n",
        "print(f\"{len(temp)}/{len(ids_df)} rows are completely broken down into radicals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Screw it, that's probably good enough, let's test it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search for å¿„ and èˆ€: 1 results\n",
            "      character components radical_string all_components\n",
            "16493         æ…†        â¿°å¿„èˆ€              å¿„             å¿„èˆ€\n",
            "\n",
            "Search for è»Š and é³¥: 1 results\n",
            "      character components radical_string all_components\n",
            "27368         é¿‚        â¿°è»Šé³¥             è»Šé³¥             è»Šé³¥\n",
            "\n",
            "âœ… Saved 99016 rows to marina_tables/ids_with_radicals.csv\n"
          ]
        }
      ],
      "source": [
        "# Cut down to needed columns (keep all_components for searching)\n",
        "ids_df = ids_df[['character', 'components', 'radical_string', 'all_components']].copy()\n",
        "\n",
        "# Search in all_components (which contains all radicals and components, not just radicals)\n",
        "# This allows us to find components like èˆ€ and å¿„ that might not be radicals themselves\n",
        "a = ids_df[\n",
        "    ids_df['all_components'].str.contains('å¿„', na=False) &\n",
        "    ids_df['all_components'].str.contains('èˆ€', na=False)\n",
        "]\n",
        "print(f\"Search for å¿„ and èˆ€: {len(a)} results\")\n",
        "print(a)\n",
        "\n",
        "b = ids_df[\n",
        "    ids_df['all_components'].str.contains('è»Š', na=False) &\n",
        "    ids_df['all_components'].str.contains('é³¥', na=False)\n",
        "]\n",
        "print(f\"\\nSearch for è»Š and é³¥: {len(b)} results\")\n",
        "print(b)\n",
        "\n",
        "# Save the final results for analysis in the next notebook\n",
        "import os\n",
        "\n",
        "folder_path = \"../marina_tables\"\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "# Save the final merged table with all our work\n",
        "ids_df.to_csv(os.path.join(folder_path, 'ids_with_radicals.csv'), \n",
        "              index=False, encoding='utf-8')\n",
        "print(f\"\\nâœ… Saved {len(ids_df)} rows to marina_tables/ids_with_radicals.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Awesome! Now you have a column that lists all the radical components for each character, going 2-3 levels deep. \n",
        "\n",
        "## What We Learned\n",
        "\n",
        "1. **Inner Join**: Only keeps rows where the key exists in BOTH tables\n",
        "2. **Left Join**: Keeps all rows from the LEFT table, adds matching data from RIGHT\n",
        "3. **Right Join**: Keeps all rows from the RIGHT table, adds matching data from LEFT\n",
        "4. **Self-Join**: Merge a table onto itself to find relationships within the same data\n",
        "\n",
        "## Summary\n",
        "\n",
        "| Merge Type | `how=` | Keeps Rows From | Use When |\n",
        "|------------|--------|------------------|----------|\n",
        "| Inner | `'inner'` | Both DataFrames | You only want matching records |\n",
        "| Left | `'left'` | Left DataFrame | Keep all from left, add matching from right |\n",
        "| Right | `'right'` | Right DataFrame | Keep all from right, add matching from left |\n",
        "\n",
        "## Key Parameters\n",
        "\n",
        "- `on='column'` - Merge on a column with the same name in both DataFrames\n",
        "- `left_on='col1', right_on='col2'` - Merge on columns with different names\n",
        "- `how='inner'` - Type of join (inner, left, right, outer)\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "In the next notebook, we'll learn how to:\n",
        "- Transform and manipulate data\n",
        "- Create new columns\n",
        "- Build lookup tables for dictionaries and character composers\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vanilla",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
