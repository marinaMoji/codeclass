{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03. Filtering Data\n",
        "\n",
        "Filtering is one of the most important skills in data analysis. You'll use it constantly to find specific characters, variants, or relationships in your data.\n",
        "\n",
        "You can do glorious things filtering data. First, however, you will be un-fucking things that don't work until 4 A.M.\n",
        "\n",
        "Let's load the stroke count table that Daniel kindly converted to CSV from the cjkvi-ids repo and print the head:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data to work with\n",
        "df = strokes_df = pd.read_csv('../daniel_tables/stroke_count_df.csv',\n",
        "    index_col=None,\n",
        "    encoding='utf-8')\n",
        "\n",
        "# Print the head !!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, one awesome thing about Pandas is that you can operate across the whole dataframe in no time. Check this out, just for fun, Daniel will add a new column where he puts a unicorn around each character:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unicorns! Yay!\n",
        "df['unicorn'] = 'ü¶Ñ' + df['character'] + 'ü¶Ñ'\n",
        "\n",
        "# Print random sample of 10 rows\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Coding is fun! It's all rainbows and unicorns, you know, Marina. And now it's your turn to try something fun! Add a new column in which you add 10 to each stroke count:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return to normal rows\n",
        "df = df[['code_point', 'character', 'stroke_count']]\n",
        "\n",
        "# Make a new column to add 10 to each stroke count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "WTF? What do you mean `TypeError: can only concatenate str (not \"int\") to str`!? Why would a column of numbers be a string (str)? Let's verify this and try to fix it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check data types\n",
        "print(df.dtypes)\n",
        "\n",
        "# Print a random sample of 10 rows\n",
        "print(df.sample(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OK, so `stroke_count` is an 'object' (string), and if you pull a couple random samples, you'll see why: there are commas and alternate stroke counts in this column, and that is why it is registering as a string. Now it's time to play cleaning lady."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Boolean Indexing\n",
        "\n",
        "Boolean indexing uses True/False conditions to select rows. This is the most common way to filter data in Pandas.\n",
        "\n",
        "Here are some cool and interesting examples of what you will one day do with boolean indexing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create a boolean condition\n",
        "char_match = df['character'] == '©åò'\n",
        "\n",
        "# Step 2: Use the condition to filter\n",
        "df_slice = df[char_match]\n",
        "\n",
        "print(df_slice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "radicals = [\n",
        "    '‰∏Ä', 'ÔΩú', '‰∏∂', '„Éé', '‰πô', '‰∫Ö', '‰∫å', '‰∫†', '‰∫∫', '‚∫Ö', '†Ü¢', 'ÂÑø', 'ÂÖ•', '„Éè', '‰∏∑', 'ÂÜÇ', \n",
        "    'ÂÜñ', 'ÂÜ´', 'Âá†', 'Âáµ', 'ÂàÄ', '‚∫â', 'Âäõ', 'Âãπ', 'Âåï', 'Âåö', 'ÂçÅ', 'Âçú', 'Âç©', 'ÂéÇ', 'Âé∂', 'Âèà', \n",
        "    '„Éû', '‰πù', '„É¶', '‰πÉ', '†Çâ', '‚ªå', 'Âè£', 'Âõó', 'Âúü', 'Â£´', 'Â§Ç', 'Â§ï', 'Â§ß', 'Â•≥', 'Â≠ê', 'ÂÆÄ', \n",
        "    'ÂØ∏', 'Â∞è', '‚∫å', 'Â∞¢', 'Â∞∏', 'Â±Æ', 'Â±±', 'Â∑ù', 'Â∑õ', 'Â∑•', 'Â∑≤', 'Â∑æ', 'Âπ≤', 'Âπ∫', 'Âπø', 'Âª¥', \n",
        "    'Âªæ', 'Âºã', 'Âºì', '„É®', 'ÂΩë', 'ÂΩ°', 'ÂΩ≥', '‚∫ñ', '‚∫ò', '‚∫°', '‚∫®', '‚∫æ', '‚ªè‚ªñ', '‰πü', '‰∫°', 'Âèä', \n",
        "    '‰πÖ', '‚∫π', 'ÂøÉ', 'Êàà', 'Êà∏', 'Êâã', 'ÊîØ', 'Êîµ', 'Êñá', 'Êñó', 'Êñ§', 'Êñπ', 'Êó†', 'Êó•', 'Êõ∞', 'Êúà', \n",
        "    'Êú®', 'Ê¨†', 'Ê≠¢', 'Ê≠π', 'ÊÆ≥', 'ÊØî', 'ÊØõ', 'Ê∞è', 'Ê∞î', 'Ê∞¥', 'ÁÅ´', '‚∫£', 'Áà™', 'Áà∂', 'Áàª', 'Áàø', \n",
        "    'Áâá', 'Áâõ', 'Áä¨', '‚∫≠', 'Áéã', 'ÂÖÉ', '‰∫ï', 'Âãø', 'Â∞§', '‰∫î', 'Â±Ø', 'Â∑¥', 'ÊØã', 'ÁéÑ', 'Áì¶', 'Áîò', \n",
        "    'Áîü', 'Áî®', 'Áî∞', 'Áñã', 'Áñí', 'Áô∂', 'ÁôΩ', 'ÁöÆ', 'Áöø', 'ÁõÆ', 'Áüõ', 'Áü¢', 'Áü≥', 'Á§∫', 'Á¶∏', 'Á¶æ', \n",
        "    'Á©¥', 'Á´ã', '‚ªÇ', '‰∏ñ', 'Â∑®', 'ÂÜä', 'ÊØç', '‚∫≤', 'Áâô', 'Áìú', 'Á´π', 'Á±≥', 'Á≥∏', 'Áº∂', 'Áæä', 'ÁæΩ', \n",
        "    'ËÄå', 'ËÄí', 'ËÄ≥', 'ËÅø', 'ËÇâ', 'Ëá™', 'Ëá≥', 'Ëáº', 'Ëàå', 'Ëàü', 'ËâÆ', 'Ëâ≤', 'Ëôç', 'Ëô´', 'Ë°Ä', 'Ë°å', \n",
        "    'Ë°£', 'Ë•ø', 'Ëá£', 'Ë¶ã', 'Ëßí', 'Ë®Ä', 'Ë∞∑', 'Ë±Ü', 'Ë±ï', 'Ë±∏', 'Ë≤ù', 'Ëµ§', 'Ëµ∞', 'Ë∂≥', 'Ë∫´', 'Ëªä', \n",
        "    'Ëæõ', 'Ëæ∞', 'ÈÖâ', 'ÈáÜ', 'Èáå', 'Ëàõ', 'È∫¶', 'Èáë', 'Èï∑', 'ÈñÄ', 'Èö∂', 'Èöπ', 'Èõ®', 'Èùí', 'Èùû', 'Â•Ñ', \n",
        "    'Â≤°', 'ÂÖç', 'Êñâ', 'Èù¢', 'Èù©', 'Èü≠', 'Èü≥', 'È†Å', 'È¢®', 'È£õ', 'È£ü', 'È¶ñ', 'È¶ô', 'ÂìÅ', 'È¶¨', 'È™®', \n",
        "    'È´ò', 'È´ü', 'È¨•', 'È¨Ø', 'È¨≤', 'È¨º', 'Á´ú', 'Èüã', 'È≠ö', 'È≥•', 'Èπµ', 'Èπø', 'È∫ª', '‰∫Ä', 'Âïá', 'ÈªÑ', \n",
        "    'Èªí', 'Èªç', 'Èªπ', 'ÁÑ°', 'Ê≠Ø', 'ÈªΩ', 'Èºé', 'Èºì', 'Èº†', 'Èºª', 'ÈΩä', 'Èæ†'\n",
        "]\n",
        "\n",
        "# Step 1: Be a boss and put your boolean condition right in the damned brakets\n",
        "df_slice = df[df['character'].isin(radicals)]\n",
        "\n",
        "print(df_slice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HEY MARINA!!!!! Stop daydreaming, you are a cleaning lady, go fix the damned commas. Let me help: \n",
        "\n",
        "- `df['character'].str` treats the string as a string.\n",
        "- `str.contains()` returns a boolean \n",
        "- placing `~` before a condition applies a negative filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new DataFrame, df_good, with the characters that don't have commas. \n",
        "# Place .copy() at the end to make it a copy (= new DataFrame)\n",
        "\n",
        "# Create a new DataFrame, df_bad, with the characters that do have commas. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Manipulating columns and data types\n",
        "\n",
        "Now, `df_good` is easy to fix. We'll just change the datatype on the row of strings comprised entirely of integers. I'll let you choose how to do this:\n",
        "\n",
        "- `df_good['stroke_count'] = df_good['stroke_count'].astype(int)`\n",
        "- `df_good = df_good.astype({'stroke_count': 'int32'})`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix df_good stroke_count column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regex\n",
        "\n",
        "Now what do we do with all those rows where there are different numbers of strokes in `df_bad`? Brilliant idea Marina, instead of choosing, let's establish a range!\n",
        "\n",
        "There are several ways to do this, but let's pull the first and last numbers out into separate columns using regex, because regex is metal. \n",
        "\n",
        "ü¶Ñ WAAAAAAAAAAAAAAAAAGH\n",
        "\n",
        "If you remember, `\\d` is a number, `+` means 'one or more', `^` signals the beginning of the string, and `$` the end, and `()` signals the group you want to pull.\n",
        "\n",
        "You can use regex with a Pandas column via `.str.extract()`.\n",
        "\n",
        "And if you're really smart, you'll slap `.astype(int)' at the end, to convert while you go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Do it, I believe in you\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Copying, renaming, and concatenating\n",
        "\n",
        "Now you have a min and max column in `df_bad`, but only 'stroke_count' in `df_good`, so you need to rename that column over there\n",
        "\n",
        "- `df = df.rename({'old': 'new'})`\n",
        "\n",
        "and you'll also need to duplicate it \n",
        "\n",
        "- `df['b'] = df['a']`\n",
        "\n",
        "Then, when you're all done, you'll need to 'concatenate' the two dataframes together:\n",
        "\n",
        "- `df = pd.concat([df_good, df_bad])`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Yay, Marina! You're a cleaning lady extraordinaire!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exporting\n",
        "\n",
        "I bet that you don't want to do that bullshit again, so export the clean DataFrame to CSV so that we can load it up later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "folder_path = \"../marina_tables\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "    print(f\"Folder '{folder_path}' created.\")\n",
        "else:\n",
        "    print(f\"Folder '{folder_path}' already exists.\")\n",
        "\n",
        "# Export the clean DataFrame to CSV\n",
        "file_path = os.path.join(folder_path, 'stroke_count_clean.csv')\n",
        "df.to_csv(file_path, index=False)\n",
        "\n",
        "print(f\"DataFrame saved to {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Conditions\n",
        "\n",
        "The stroke count table is obviously the last that we'll have to clean any data for the marinaMoji project, so now we can do something really chill and rewarding: check out the giant IDS character composition table! \n",
        "\n",
        "Let's load that up and check out what we're dealing with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load IDS data as DataFrame\n",
        "ids_df = pd.read_csv('../daniel_tables/ids_df.csv',\n",
        "    index_col=None,\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "# Print the head of the IDs DataFrame\n",
        "\n",
        "# Print the shape of the IDs DataFrame\n",
        "print(ids_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "313,060 rows! Holy shit!\n",
        "\n",
        "\n",
        "### Using AND (&) and OR (|)\n",
        "\n",
        "Let's do some *avanced* boolean filtering for this one. Daniel is a complicated man, who surrounds himself with complicated women ‚Äì the sort who ask for a cup of milk, then put all their potatoes in it, give it back to you, then announce 'voil√†'. To build on this, let's filter for characters featuring Â•≥ with more than 12 strokes. \n",
        "\n",
        "**Important**: When using multiple conditions, you must wrap each condition in parentheses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter stroke count DataFrame for characters with more than 12 strokes\n",
        "df_slice = strokes_df[strokes_df['stroke_count'] > 12]\n",
        "\n",
        "# Pull a list of the characters\n",
        "list_of_characters = df_slice['character'].tolist()\n",
        "\n",
        "# Filter the IDs DataFrame accordingly\n",
        "df_slice = ids_df[\n",
        "    (ids_df['components'].str.contains('Â•≥', na=False)) & \n",
        "    (ids_df['character'].isin(list_of_characters))\n",
        "]\n",
        "\n",
        "print(f\"Filtered IDs DataFrame: {len(df_slice)} rows\")\n",
        "print(df_slice.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, everything is all rainbows and unicorns again! Man, can you believe that we have a table of 313,060 characters? I didn't even know that there were that many characters! This is such great news, this table must be entirely perfect and complete at that size, and now we can relax.\n",
        "\n",
        "OK, Marina, now it's finally time for you to show the data who's the boss. Let's start by looking for a fun character:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the DataFrame for the character \"Âê¶\" and print the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Duplicates\n",
        "\n",
        "Mother fucker! There are duplicates!? Well, better put your cleaning lady apron back on, because this DataFrame isn't going to clean itself.\n",
        "\n",
        "To get rid of duplicates, there are several options:\n",
        "\n",
        "- Whole table: `df = df.drop_duplicates()`\n",
        "- Subset: `df = df.drop_duplicates(subset=['character', 'components'])`\n",
        "- negative filter = `df = df[~df['character'].duplicated()]`\n",
        "\n",
        "Look online for different parameters to feed these two methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop duplicates from the DataFrame\n",
        "print(ids_df[ids_df['character'] == 'Âê¶'])\n",
        "\n",
        "# Print the DataFrame size again to see the difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oh, so half the table was duplicates... Well, at least there are over 170,000 distinct characters divided into components in our table, right?\n",
        "\n",
        "\n",
        "## Empty cells\n",
        "\n",
        "There is no way that our big beautiful DataFrame is filled with a bunch of empty cells right? Pandas uses `na` (not applicable) and the numpy object `nan` (not a number) for blanks. Specifically:\n",
        "\n",
        "- Applied to a column, the `.isna()` method returns a boolean you can use to filter\n",
        "- Applied to a DataFrame, `.dropna()` with the parameter `subset` allows you to get rid of rows with empty cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter DataFrame to show only rows with empty 'character' cells\n",
        "\n",
        "# Drop those rows\n",
        "\n",
        "# Print the DataFrame size again to see the difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ugh, well, at least everything else is fine, right? Let's see if there are any characters whose components are just the same character itself, i.e. filter by `ids_df['character'] == ids_df['components']`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       code_point character components\n",
            "0         U+04E00         ‰∏Ä          ‰∏Ä\n",
            "22        U+04E01         ‰∏Å          ‰∏Å\n",
            "62        U+04E02         ‰∏Ç          ‰∏Ç\n",
            "78        U+04E03         ‰∏É          ‰∏É\n",
            "87        U+04E0C         ‰∏å          ‰∏å\n",
            "...           ...       ...        ...\n",
            "301210    U+2E3B3         Æé≥          Æé≥\n",
            "302332    U+2E815         Æ†ï          Æ†ï\n",
            "309760    U+3193B         ±§ª          ±§ª\n",
            "310497    U+31C1C         ±∞ú          ±∞ú\n",
            "311237    U+31F00         ±ºÄ          ±ºÄ\n",
            "\n",
            "[2262 rows x 3 columns]\n",
            "                             code_point character components\n",
            "18347   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "39372   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "45955   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "46428   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "54621   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "62814   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "71007   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "79200   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "87393   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "89145   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "93295   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "93518   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "99281   ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "106755  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "107298  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "214492  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "235469  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "236012  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "236368  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "242973  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "251166  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "259359  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "267552  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "275745  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "283938  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "285690  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "289844  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "290067  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "295830  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "303304  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "308244  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n",
            "312437  ;; -*- coding: utf-8-mcs-er -*-       NaN        NaN\n"
          ]
        }
      ],
      "source": [
        "# Go, Marina, go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The good news\n",
        "\n",
        "I know what you're thinking: from the outside, digital humanities looks sexy; from the inside, it's just endless, thankless cleaning. That's not true at all, Marina! There's much more to it than that. Let's load the shin-ky√ª tables I recently found on cjkvi-variants and look up Áúü and Áúû.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     shin kyu\n",
            "1269    Áúü   Áúû\n"
          ]
        }
      ],
      "source": [
        "csv_path = '../daniel_tables/shin_kyu_df.csv'\n",
        "\n",
        "# Load the csv yourself\n",
        "shin_kyu_df = pd.read_csv(csv_path,\n",
        "    index_col=None,\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "# Print the head, to see what we're working with\n",
        "\n",
        "# Filter for kyu == Áúû\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hurray! What did this teach us?\n",
        "\n",
        "That's right: OpenCC is not giving us true Japanese shin-ky√ª conversion and, thus, we need to throw that part of our project away and build a whole new fucking conversion table and module from scratch.\n",
        "\n",
        "Yay!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Operation | Syntax | Example |\n",
        "|-----------|--------|---------|\n",
        "| Equal to | `df[df['col'] == value]` | `df[df['type'] == 'simplified']` |\n",
        "| Not equal | `df[df['col'] != value]` | `df[df['type'] != 'simplified']` |\n",
        "| AND | `df[(cond1) & (cond2)]` | `df[(df['a'] == 1) & (df['b'] == 2)]` |\n",
        "| OR | `df[(cond1) \\| (cond2)]` | `df[(df['a'] == 1) \\| (df['b'] == 2)]` |\n",
        "| Contains text | `df[df['col'].str.contains('text')]` | `df[df['ids'].str.contains('‰∫∫')]` |\n",
        "| Starts with | `df[df['col'].str.startswith('text')]` | `df[df['ids'].str.startswith('‚ø∞')]` |\n",
        "| Not null | `df[df['col'].notna()]` | `df[df['variant'].notna()]` |\n",
        "| Is null | `df[df['col'].isna()]` | `df[df['variant'].isna()]` |\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "In the next notebook, we'll learn the most critical skill for your work:\n",
        "- **Merging datasets** - Combining data from multiple sources\n",
        "- This is essential for building comprehensive character lookup tables!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vanilla",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
